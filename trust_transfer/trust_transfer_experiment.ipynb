{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import trust_transfer_experiment_utils\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import llm_utils\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "task_descr = {\n",
    "    'household': [\n",
    "        ' ',\n",
    "        'Pick and place a glass',\n",
    "        'Pick and place a plastic can',\n",
    "        'Pick and place a lemon',\n",
    "        'Pick and place a plastic bottle',\n",
    "        'Pick and place an apple',\n",
    "        'Pick and place a plastic cup',\n",
    "        'Navigate while avoiding moving people',\n",
    "        'Navigate to the main room door',\n",
    "        'Navigate while following a person',\n",
    "        'Navigate to the dining table',\n",
    "        'Navigate while avoiding obstacles',\n",
    "        'Navigate to the living room'\n",
    "    ],\n",
    "    'driving': [\n",
    "        ' ',\n",
    "        'Parking backwards cars and people around, misaligned',\n",
    "        'Parking backwards empty lot, misaligned',\n",
    "        'Parking backwards cars and people around, aligned',\n",
    "        'Parking forwards empty lot, aligned',\n",
    "        'Parking forwards cars and people around, misaligned',\n",
    "        'Parking forwards empty lot, misaligned',\n",
    "        'Navigating lane merge with other moving vehicles',\n",
    "        'Navigating lane merge on a clear road',\n",
    "        'Navigating traffic-circle with other moving vehicles',\n",
    "        'Navigating traffic-circle on a clear road',\n",
    "        'Navigating T-junction with other moving vehicles',\n",
    "        'Navigating T-junction on a clear road',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driving_csv_path = './data/trust_transfer_driving_cleaned.csv'\n",
    "household_csv_path = './data/trust_transfer_household_cleaned.csv'\n",
    "\n",
    "trust_transfer_driving_df = pd.read_csv(driving_csv_path)\n",
    "trust_transfer_household_df = pd.read_csv(household_csv_path)\n",
    "\n",
    "dom_list = ['household', 'driving']\n",
    "prompt_structure_list = ['base', 'altered']\n",
    "\n",
    "answer_choices = [str(i) for i in range(1, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_loaded = False\n",
    "for dom in dom_list:\n",
    "    for prompt_structure in prompt_structure_list:\n",
    "        post_obs_trust_csv_path = f'./results/trust_transfer_t5_post_obs_{dom}_{prompt_structure}.csv'\n",
    "        if not os.path.exists(post_obs_trust_csv_path):\n",
    "            # Load model\n",
    "            if not model_loaded:\n",
    "                max_memory = {0: \"20GIB\", 1: \"20GIB\", 2: \"20GIB\", 3: \"20GIB\"}\n",
    "                t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\", max_memory=max_memory)\n",
    "                t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "                model_loaded = True\n",
    "\n",
    "            post_obs_trust_csv_file = open(post_obs_trust_csv_path, 'w')\n",
    "            writer = csv.writer(post_obs_trust_csv_file)\n",
    "            header = ['id', 'test_task_descr_1', 'test_task_descr_2', 'test_task_descr_3', 'obs_task_descr_1', 'obs_task_descr_2', 'obs_tasks_perf_1', 'obs_tasks_perf_2', 'post_obs_task_descr', 'prompt']\n",
    "            header += answer_choices\n",
    "            writer.writerow(header)\n",
    "\n",
    "            if dom == 'household':\n",
    "                df = trust_transfer_household_df\n",
    "                if prompt_structure == 'base':\n",
    "                    prompt_creation_fn = trust_transfer_experiment_utils.create_template_household_t5\n",
    "                else:\n",
    "                    prompt_creation_fn = trust_transfer_experiment_utils.create_template_household_t5_altered\n",
    "            else:\n",
    "                df = trust_transfer_driving_df\n",
    "                if prompt_structure == 'base':\n",
    "                    prompt_creation_fn = trust_transfer_experiment_utils.create_template_driving_t5\n",
    "                else:\n",
    "                    prompt_creation_fn = trust_transfer_experiment_utils.create_template_driving_t5_altered\n",
    "\n",
    "            for row_idx, row in tqdm(df.iterrows()):\n",
    "                test_tasks_id = [int(row['C_ID']), int(row['D_ID']), int(row['E_ID'])]\n",
    "                test_tasks_descr = [task_descr[dom][test_task_id] for test_task_id in test_tasks_id]\n",
    "                pre_trust = [int(row['C1_rating']), int(row['D1_rating']), int(row['E1_rating'])]\n",
    "                obs_tasks_id = [int(row['A_ID']), int(row['B_ID'])]\n",
    "                obs_tasks_perf = [True, True] if int(row['B_SF']) else [False, False]\n",
    "                obs_tasks_descr = [task_descr[dom][obs_task_id] for obs_task_id in obs_tasks_id]\n",
    "\n",
    "                for idx in range(3):\n",
    "                    template = prompt_creation_fn(test_tasks_descr, pre_trust, obs_tasks_descr, obs_tasks_perf, test_tasks_descr[idx])\n",
    "                    probs = llm_utils.get_probs_t5([template], answer_choices, t5_model, t5_tokenizer)[0]\n",
    "                    writer.writerow([row_idx] + test_tasks_descr + obs_tasks_descr + obs_tasks_perf + [test_tasks_descr[idx], template] + probs)\n",
    "            post_obs_trust_csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dom in dom_list:\n",
    "    for prompt_structure in prompt_structure_list:\n",
    "        post_obs_trust_csv_path = f'./results/trust_transfer_davinci_post_obs_{dom}_{prompt_structure}.csv'\n",
    "        if not os.path.exists(post_obs_trust_csv_path):\n",
    "            post_obs_trust_csv_file = open(post_obs_trust_csv_path, 'w')\n",
    "            writer = csv.writer(post_obs_trust_csv_file)\n",
    "            header = ['id', 'test_task_descr_1', 'test_task_descr_2', 'test_task_descr_3', 'obs_task_descr_1', 'obs_task_descr_2', 'obs_tasks_perf_1', 'obs_tasks_perf_2', 'post_obs_task_descr', 'prompt']\n",
    "            header += answer_choices\n",
    "            writer.writerow(header)\n",
    "\n",
    "            if dom == 'household':\n",
    "                df = trust_transfer_household_df\n",
    "                if prompt_structure == 'base':\n",
    "                    prompt_creation_fn = trust_transfer_experiment_utils.create_template_household_davinci()\n",
    "                else:\n",
    "                    prompt_creation_fn = trust_transfer_experiment_utils.create_template_household_davinci_altered\n",
    "            else:\n",
    "                df = trust_transfer_driving_df\n",
    "                if prompt_structure == 'base':\n",
    "                    prompt_creation_fn = trust_transfer_experiment_utils.create_template_driving_davinci\n",
    "                else:\n",
    "                    prompt_creation_fn = trust_transfer_experiment_utils.create_template_driving_davinci_altered\n",
    "\n",
    "            for row_idx, row in tqdm(df.iterrows()):\n",
    "                test_tasks_id = [int(row['C_ID']), int(row['D_ID']), int(row['E_ID'])]\n",
    "                test_tasks_descr = [task_descr[dom][test_task_id] for test_task_id in test_tasks_id]\n",
    "                pre_trust = [int(row['C1_rating']), int(row['D1_rating']), int(row['E1_rating'])]\n",
    "                obs_tasks_id = [int(row['A_ID']), int(row['B_ID'])]\n",
    "                obs_tasks_perf = [True, True] if int(row['B_SF']) else [False, False]\n",
    "                obs_tasks_descr = [task_descr[dom][obs_task_id] for obs_task_id in obs_tasks_id]\n",
    "\n",
    "                for idx in range(3):\n",
    "                    template = prompt_creation_fn(test_tasks_descr, pre_trust, obs_tasks_descr, obs_tasks_perf, test_tasks_descr[idx])\n",
    "                    probs = llm_utils.get_probs_davinci(template, answer_choices)\n",
    "                    writer.writerow([row_idx] + test_tasks_descr + obs_tasks_descr + obs_tasks_perf + [test_tasks_descr[idx], template] + probs)\n",
    "            post_obs_trust_csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 RESULTS:\n",
      "\n",
      "Post same as Pre 21\n",
      "Domain household Prompt Structure base MAE 0.21115117215210447, CwM 0.7604166666666666\n",
      "Post same as Pre 23\n",
      "Domain driving Prompt Structure base MAE 0.21062865374388773, CwM 0.7204301075268817\n",
      "Overall MAE 0.2108940599194899, CwM 0.7407407407407407\n",
      "------------------------------\n",
      "Post same as Pre 2\n",
      "Domain household Prompt Structure altered MAE 0.16952754448010476, CwM 0.8020833333333334\n",
      "Post same as Pre 3\n",
      "Domain driving Prompt Structure altered MAE 0.15976733711975816, CwM 0.7956989247311828\n",
      "Overall MAE 0.1647249027631088, CwM 0.798941798941799\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('T5 RESULTS:\\n')\n",
    "for prompt_structure in prompt_structure_list:\n",
    "    mae_list = []\n",
    "    cwm_list = []\n",
    "    for dom in dom_list:\n",
    "        post_obs_trust_csv_path = f'./results/trust_transfer_t5_post_obs_{dom}_{prompt_structure}.csv'\n",
    "        llm_post_trust_df = pd.read_csv(post_obs_trust_csv_path)\n",
    "        if dom == 'household':\n",
    "            gt_df = trust_transfer_household_df\n",
    "        else:\n",
    "            gt_df = trust_transfer_driving_df\n",
    "        mae, cwm = trust_transfer_experiment_utils.analyze_results(llm_post_trust_df, gt_df)\n",
    "        print(f\"Domain {dom} Prompt Structure {prompt_structure} MAE {mae}, CwM {cwm}\")\n",
    "        mae_list.append(mae)\n",
    "        cwm_list.append(cwm)\n",
    "    print(f\"Overall MAE {(mae_list[0] * 96 + mae_list[1] * 93) / 189}, CwM {(cwm_list[0] * 96 + cwm_list[1] * 93) / 189}\")\n",
    "    print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davinci RESULTS:\n",
      "\n",
      "Post same as Pre 0\n",
      "Domain household Prompt Structure base MAE 0.1621312900357, CwM 0.8958333333333334\n",
      "Post same as Pre 0\n",
      "Domain driving Prompt Structure base MAE 0.15420239514865774, CwM 0.8064516129032258\n",
      "Overall MAE 0.1582297703293776, CwM 0.8518518518518519\n",
      "------------------------------\n",
      "Post same as Pre 1\n",
      "Domain household Prompt Structure altered MAE 0.16303994685915713, CwM 0.875\n",
      "Post same as Pre 0\n",
      "Domain driving Prompt Structure altered MAE 0.15503605938460954, CwM 0.8064516129032258\n",
      "Overall MAE 0.15910152603834798, CwM 0.8412698412698413\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Davinci RESULTS:\\n')\n",
    "for prompt_structure in prompt_structure_list:\n",
    "    mae_list = []\n",
    "    cwm_list = []\n",
    "    for dom in dom_list:\n",
    "        post_obs_trust_csv_path = f'./results/trust_transfer_davinci_post_obs_{dom}_{prompt_structure}.csv'\n",
    "        llm_post_trust_df = pd.read_csv(post_obs_trust_csv_path)\n",
    "        if dom == 'household':\n",
    "            gt_df = trust_transfer_household_df\n",
    "        else:\n",
    "            gt_df = trust_transfer_driving_df\n",
    "        mae, cwm = trust_transfer_experiment_utils.analyze_results(llm_post_trust_df, gt_df)\n",
    "        print(f\"Domain {dom} Prompt Structure {prompt_structure} MAE {mae}, CwM {cwm}\")\n",
    "        mae_list.append(mae)\n",
    "        cwm_list.append(cwm)\n",
    "    print(f\"Overall MAE {(mae_list[0] * 96 + mae_list[1] * 93) / 189}, CwM {(cwm_list[0] * 96 + cwm_list[1] * 93) / 189}\")\n",
    "    print('------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
